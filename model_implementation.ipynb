{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 582
    },
    "id": "uh7dbecb7sOC",
    "outputId": "7f081707-9d73-4989-a9ae-0e1411fe71e4"
   },
   "outputs": [],
   "source": [
    "# Install google cloud storage package if you haven't already\n",
    "!pip install google-cloud-storage==2.0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 73
    },
    "id": "RjE7xY8IxxhO",
    "outputId": "5c1f1711-02b2-4fcf-eec5-fa47d3051ace"
   },
   "outputs": [],
   "source": [
    "# Upload GCS key to file system\n",
    "from google.colab import files\n",
    "uploaded = files.upload()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FnZ-PFgG7fsz",
    "outputId": "42f1929a-1259-4203-90bc-966a844e5572"
   },
   "outputs": [],
   "source": [
    "# Verify that the key was uploaded\n",
    "!ls /content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZXt5c6AJ49HI"
   },
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import os\n",
    "import torch\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, models\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "from google.cloud import storage\n",
    "from io import BytesIO\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CI1qxXUe5JwE"
   },
   "outputs": [],
   "source": [
    "# Use a service account key for long-life credentials\n",
    "os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"] = \"/content/my-gcs-key.json\"  # Replace with your service account key path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "C6RRIxuc4goq",
    "outputId": "1699a21c-7ef3-4ff4-a4f9-069384ec9f99"
   },
   "outputs": [],
   "source": [
    "# Set random seed for reproducibility\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# Set the device to GPU if available, otherwise CPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "client = storage.Client()\n",
    "bucket_name = 'vindr-mammo-dataset'  # Replace with your bucket name\n",
    "bucket = client.bucket(bucket_name)\n",
    "\n",
    "# Define the custom dataset class\n",
    "class VindrMammoDataset(Dataset):\n",
    "    def __init__(self, bucket, dataframe, transform=None):\n",
    "        self.bucket = bucket        # Google Cloud Storage bucket\n",
    "        self.dataframe = dataframe  # Dataframe containing image filenames and bi-rads labels\n",
    "        self.transform = transform  # Transform for data augmentation\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataframe)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image_id = self.dataframe.iloc[idx, 0]\n",
    "        breast_birads = self.dataframe.iloc[idx, 1]\n",
    "\n",
    "        # Extract the number from the BI-RADS rating\n",
    "        birads_value = int(breast_birads.split()[-1])\n",
    "        # Map BI-RADS values to binary classes\n",
    "        if birads_value in [1, 2, 3]:  # Benign\n",
    "            label = 0\n",
    "        else:  # Malignant (BI-RADS 4, 5)\n",
    "            label = 1\n",
    "\n",
    "        # Concatenate the path to the image file in GCS bucket\n",
    "        img_path = f\"images/{image_id}.png\"\n",
    "\n",
    "        # Load the image from the GCS bucket\n",
    "        blob = self.bucket.blob(img_path)\n",
    "        image_data = blob.download_as_bytes()\n",
    "        image = Image.open(BytesIO(image_data)).convert('RGB') # Ensure that it's RGB\n",
    "\n",
    "        # Apply transformations (if there are any)\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image, label\n",
    "\n",
    "\n",
    "# Define the image transformations for dynamic preprocessing as data is loaded\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((256, 256)),  # Ensure that images are 256x256\n",
    "    transforms.ToTensor(),  # Convert images to PyTorch tensors\n",
    "])\n",
    "\n",
    "# Load the finding_annotations.csv file from GCS\n",
    "csv_blob = bucket.blob(\"finding_annotations.csv\")\n",
    "csv_data = csv_blob.download_as_text()\n",
    "annotations_df = pd.read_csv(BytesIO(csv_data.encode()))\n",
    "\n",
    "# Filter the DataFrame for training and test sets\n",
    "train_df = annotations_df[annotations_df['split'] == 'training']\n",
    "test_df = annotations_df[annotations_df['split'] == 'test']\n",
    "\n",
    "# Create a new DataFrame with only the necessary columns\n",
    "train_df = train_df[['image_id', 'breast_birads']]\n",
    "test_df = test_df[['image_id', 'breast_birads']]\n",
    "\n",
    "# Reset the index for both DataFrames\n",
    "train_df = train_df.reset_index(drop=True)\n",
    "test_df = test_df.reset_index(drop=True)\n",
    "\n",
    "# Display the first few rows of the training DataFrame\n",
    "print(\"Training DataFrame:\")\n",
    "print(train_df.head())\n",
    "\n",
    "# Create the datasets\n",
    "train_dataset = VindrMammoDataset(bucket=bucket, dataframe=train_df, transform=transform)\n",
    "test_dataset = VindrMammoDataset(bucket=bucket, dataframe=test_df, transform=transform)\n",
    "\n",
    "# Create the data loaders\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=32, shuffle=True, num_workers=4, pin_memory=True)\n",
    "valid_loader = torch.utils.data.DataLoader(test_dataset, batch_size=32, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fjmA6TnnQFZt"
   },
   "source": [
    "## VGG16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XcXwYyNdCP4b",
    "outputId": "7bf4ed76-0fbd-4b9c-d740-366c6e950abf"
   },
   "outputs": [],
   "source": [
    "# Build pre-trained VGG16 model\n",
    "def build_vgg_model(num_classes=2):  # Binary classification\n",
    "    vgg16 = models.vgg16(weights=models.VGG16_Weights.IMAGENET1K_V1)\n",
    "\n",
    "    # Replace the final classification layer\n",
    "    vgg16.classifier[6] = nn.Linear(in_features=4096, out_features=num_classes)\n",
    "    return vgg16\n",
    "\n",
    "vgg16 = build_vgg_model(num_classes=2)  # Set num_classes to 2 for binary classification\n",
    "\n",
    "# Move the model to the appropriate device (GPU if available)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "vgg16 = vgg16.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 373
    },
    "id": "p8u-U9fgGdAi",
    "outputId": "7df478a5-b16b-4cbf-fab3-edf702eefc98"
   },
   "outputs": [],
   "source": [
    "# Define the loss function and optimizer, the optimizer requires model parameters and a learning rate. 0.0001 is typical\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(vgg16.parameters(), lr=1e-4)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "    vgg16.train()\n",
    "    running_loss = 0.0\n",
    "\n",
    "    for images, labels in tqdm(train_loader, desc=f'Epoch {epoch+1}/{num_epochs}'):\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = vgg16(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item() * images.size(0)\n",
    "\n",
    "    epoch_loss = running_loss / len(train_loader.dataset)\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {epoch_loss:.4f}\")\n",
    "\n",
    "    # Save the model after each epoch for performance evaluation and comparison\n",
    "    timestamp = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    vgg16_model_path = f\"vgg16_breast_cancer_detect_{timestamp}.pth\"  # Define the path to save the model\n",
    "    torch.save(vgg16.state_dict(), vgg16_model_path)  # Save the model's weights\n",
    "\n",
    "    print(f\"Model saved after epoch {epoch+1} to {vgg16_model_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true,
     "base_uri": "https://localhost:8080/"
    },
    "id": "UeeiZNAJCRiS",
    "outputId": "ad7f550e-f093-40e1-997e-d8ab75f4155e"
   },
   "outputs": [],
   "source": [
    "# Initial Accuracy Evaluation\n",
    "vgg16.eval()  # Set the model to evaluation mode\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for images, labels in tqdm(valid_loader, desc='Evaluating'):\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        outputs = vgg16(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "accuracy = 100 * correct / total\n",
    "print(f'Accuracy of the model on the test images: {accuracy:.2f}%')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1fFhCV7eQKuE"
   },
   "source": [
    "## MobileNetV2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1sVvRx0xQO3B",
    "outputId": "6368f893-9bd0-49f3-ef4f-0bd3784cbe79"
   },
   "outputs": [],
   "source": [
    "# Build pre-trained MobileNetV2 model\n",
    "def build_mobilenetv2_model(num_classes=2):  # Binary classification\n",
    "    mobilenetv2 = models.mobilenet_v2(pretrained=True)\n",
    "\n",
    "    # Replace the final classification layer\n",
    "    mobilenetv2.classifier[1] = nn.Linear(in_features=1280, out_features=num_classes)\n",
    "    return mobilenetv2\n",
    "\n",
    "mobilenetv2 = build_mobilenetv2_model(num_classes=2)  # Set num_classes to 2 for binary classification\n",
    "\n",
    "# Move the model to the appropriate device (GPU if available)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "mobilenetv2 = mobilenetv2.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yT2QKS8lSgVS",
    "outputId": "6471d9ab-1d7d-4622-cbf9-fbad40b73202"
   },
   "outputs": [],
   "source": [
    "# Define the loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(mobilenetv2.parameters(), lr=1e-4)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "    mobilenetv2.train()\n",
    "    running_loss = 0.0\n",
    "\n",
    "    for images, labels in tqdm(train_loader, desc=f'Epoch {epoch+1}/{num_epochs}'):\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = mobilenetv2(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item() * images.size(0)\n",
    "\n",
    "    epoch_loss = running_loss / len(train_loader.dataset)\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {epoch_loss:.4f}\")\n",
    "\n",
    "# Release variables from GPU memory to avoid unnecessary GPU usage - reduce cost and compute power.\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# Save the model for performance evaluation and comparison\n",
    "timestamp = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "mobilenetv2_model_path = f\"mobilenetv2_breast_cancer_detect_{timestamp}.pth\"  # Define the path to save the model\n",
    "torch.save(mobilenetv2.state_dict(), mobilenetv2_model_path)  # Save the model's weights\n",
    "\n",
    "print(f\"Model saved to {mobilenetv2_model_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jJvlJ5MZUQ0B",
    "outputId": "89bcb1b3-938a-4f4a-cfbf-a1a73982c532"
   },
   "outputs": [],
   "source": [
    "# Initial Accuracy Evaluation\n",
    "mobilenetv2.eval()  # Set the model to evaluation mode\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for images, labels in tqdm(valid_loader, desc='Evaluating'):\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        outputs = mobilenetv2(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "accuracy = 100 * correct / total\n",
    "print(f'Accuracy of the model on the test images: {accuracy:.2f}%')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ResNet18"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "s2IxUwQgIEuL",
    "outputId": "33bcaf78-ac1f-4125-ff8d-9b1d32551dd0"
   },
   "outputs": [],
   "source": [
    "# Build pre-trained ResNet18 model\n",
    "def build_resnet_model(num_classes=2):  # Binary classification\n",
    "    resnet18 = models.resnet18(pretrained=True)\n",
    "\n",
    "    # Replace the final classification layer to match the binary classification task\n",
    "    resnet18.fc = nn.Linear(in_features=resnet18.fc.in_features, out_features=num_classes)\n",
    "    return resnet18\n",
    "\n",
    "resnet18 = build_resnet_model(num_classes=2)  # Set num_classes to 2 for binary classification\n",
    "\n",
    "# Move the model to the appropriate device (GPU if available)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "resnet18 = resnet18.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 211
    },
    "id": "e26tWi9jI-Rb",
    "outputId": "dd32ce2d-d45c-41e0-c8b6-e3eb5e0f06c8"
   },
   "outputs": [],
   "source": [
    "# Define the loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(resnet18.parameters(), lr=1e-4)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "    resnet18.train()\n",
    "    running_loss = 0.0\n",
    "\n",
    "    for images, labels in tqdm(train_loader, desc=f'Epoch {epoch+1}/{num_epochs}'):\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = resnet18(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item() * images.size(0)\n",
    "\n",
    "    epoch_loss = running_loss / len(train_loader.dataset)\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {epoch_loss:.4f}\")\n",
    "\n",
    "# Save the model for performance evaluation and comparison\n",
    "timestamp = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "resnet18_model_path = f\"resnet18_breast_cancer_detect_{timestamp}.pth\"  # Define the path to save the model\n",
    "torch.save(resnet18.state_dict(), resnet18_model_path)  # Save the model's weights\n",
    "\n",
    "print(f\"Model saved to {resnet18_model_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initial Accuracy Evaluation\n",
    "resnet18.eval()  # Set the model to evaluation mode\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for images, labels in tqdm(valid_loader, desc='Evaluating'):\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        outputs = resnet18(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "accuracy = 100 * correct / total\n",
    "print(f'Accuracy of the model on the test images: {accuracy:.2f}%')"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "A100",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
