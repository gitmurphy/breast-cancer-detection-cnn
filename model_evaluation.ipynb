{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vvHKfwTaxw7r"
   },
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import models\n",
    "\n",
    "# Set the device to GPU if available, otherwise CPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M1wqh4aT5pDW"
   },
   "source": [
    "## Import Models\n",
    "Load the VGG-16, ResNet18 and MobileNetV2 models which were created with weights learned from ImageNet, then adusted using VinDr-Mammo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "t9V-W_yJfeFw",
    "outputId": "0f40411b-8d22-4762-8e9c-84966e2429ab"
   },
   "outputs": [],
   "source": [
    "# Mount google drive to access the pre-trained models\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9mxa16D2d5Tu"
   },
   "outputs": [],
   "source": [
    "# Define paths to model state files from VinDr-Mammo training\n",
    "vgg16_model_path = 'path/to/vgg16_model.pth'\n",
    "resnet18_model_path = 'path/to/resnet18_model.pth'\n",
    "mobilenetv2_model_path = 'path/to/mobilenetv2_model.pth'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LL8WbsAmgdHB"
   },
   "outputs": [],
   "source": [
    "# Initialize models without pre-trained weights from ImageNet or VinDr-Mammo\n",
    "# Adapt for binary classification\n",
    "\n",
    "# VGG-16\n",
    "vgg16_model = models.vgg16(weights=None)\n",
    "vgg16_model.classifier[6] = nn.Linear(4096, 2)\n",
    "\n",
    "#ResNet-18\n",
    "resnet18_model = models.resnet18(weights=None)\n",
    "resnet18_model.fc = nn.Linear(resnet18_model.fc.in_features, 2)\n",
    "\n",
    "#MobileNetV2\n",
    "mobilenetv2_model = models.mobilenet_v2(weights=None)\n",
    "mobilenetv2_model.classifier[1] = nn.Linear(1280, 2)\n",
    "\n",
    "#MobileNetV2FixedWeights\n",
    "mobilenetv2_fix_birads_model = models.mobilenet_v2(weights=None)\n",
    "mobilenetv2_fix_birads_model.classifier[1] = nn.Linear(1280, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pIzW_xc3hji9",
    "outputId": "5196bd1c-0c68-4b44-d58a-6a11600cdf47"
   },
   "outputs": [],
   "source": [
    "# Load models with weights from state files, ensuring that the model is also loaded for the correct device (should be GPU)\n",
    "vgg16_model.load_state_dict(torch.load(vgg16_model_path, map_location=device))\n",
    "resnet18_model.load_state_dict(torch.load(resnet18_model_path, map_location=device))\n",
    "mobilenetv2_model.load_state_dict(torch.load(mobilenetv2_model_path, map_location=device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qABO1W-spNNo",
    "outputId": "7b6870c8-3ce3-449d-da6b-97b04bed4990"
   },
   "outputs": [],
   "source": [
    "# Move the models to the GPU\n",
    "vgg16_model.to(device)\n",
    "resnet18_model.to(device)\n",
    "mobilenetv2_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nAOk3Tt8pWMh",
    "outputId": "1a71295c-c883-4140-f1ba-ed0850303c41"
   },
   "outputs": [],
   "source": [
    "# Check if the models were successfully moved to the GPU\n",
    "print(next(vgg16_model.parameters()).device)\n",
    "print(next(resnet18_model.parameters()).device)\n",
    "print(next(mobilenetv2_model.parameters()).device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tLpPW2v56sBp"
   },
   "source": [
    "## Create Datset and DataLoader for test data\n",
    "Before setting the model to evaluation mode we will first need to create the dataset and dataloader required by the model to accept our VinDr-Mammo test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 602
    },
    "id": "rVjPBOn_8Yh8",
    "outputId": "b4d3defb-8769-40b7-86fd-bd476463272a"
   },
   "outputs": [],
   "source": [
    "# Install google cloud storage package if you haven't already\n",
    "!pip install google-cloud-storage==2.0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 73
    },
    "id": "n2MABo5P5WUB",
    "outputId": "1d94359a-4ce6-4b89-f2f3-805390e8c1fa"
   },
   "outputs": [],
   "source": [
    "# Upload GCS key to file system\n",
    "from google.colab import files\n",
    "uploaded = files.upload()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "38plszD25ISd"
   },
   "outputs": [],
   "source": [
    "# Use a service account key for long-life credentials\n",
    "import os\n",
    "os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"] = \"/content/my-gcs-key.json\"  # Replace with your service account key path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ewNY6BWj5keA"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from google.cloud import storage\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "from io import BytesIO\n",
    "\n",
    "client = storage.Client()\n",
    "bucket_name = 'vindr-mammo-dataset'  # Replace with your bucket name\n",
    "bucket = client.bucket(bucket_name)\n",
    "\n",
    "# Define the custom dataset class\n",
    "class VindrMammoDataset(Dataset):\n",
    "    def __init__(self, bucket, dataframe, transform=None):\n",
    "        self.bucket = bucket        # Google Cloud Storage bucket\n",
    "        self.dataframe = dataframe  # Dataframe containing image filenames and bi-rads labels\n",
    "        self.transform = transform  # Transform for data augmentation\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataframe)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image_id = self.dataframe.iloc[idx, 0]\n",
    "        breast_birads = self.dataframe.iloc[idx, 1]\n",
    "\n",
    "        # Extract the number from the BI-RADS rating\n",
    "        birads_value = int(breast_birads.split()[-1])\n",
    "        # Map BI-RADS values to binary classes\n",
    "        if birads_value in [1, 2, 3]:  # Benign\n",
    "            label = 0\n",
    "        else:  # Malignant (BI-RADS 4, 5)\n",
    "            label = 1\n",
    "\n",
    "        # Concatenate the path to the image file in GCS bucket\n",
    "        img_path = f\"images/{image_id}.png\"\n",
    "\n",
    "        # Load the image from the GCS bucket\n",
    "        blob = self.bucket.blob(img_path)\n",
    "        image_data = blob.download_as_bytes()\n",
    "        image = Image.open(BytesIO(image_data)).convert('RGB') # Ensure that it's RGB\n",
    "\n",
    "        # Apply transformations (if there are any)\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image, label\n",
    "\n",
    "\n",
    "# Define the image transformations for dynamic preprocessing as data is loaded\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((256, 256)),  # Ensure that images are 256x256\n",
    "    transforms.ToTensor(),  # Convert images to PyTorch tensors\n",
    "])\n",
    "\n",
    "# Load the finding_annotations.csv file from GCS\n",
    "csv_blob = bucket.blob(\"finding_annotations.csv\")\n",
    "csv_data = csv_blob.download_as_text()\n",
    "annotations_df = pd.read_csv(BytesIO(csv_data.encode()))\n",
    "\n",
    "# Filter the DataFrame for the test sets\n",
    "test_df = annotations_df[annotations_df['split'] == 'test']\n",
    "\n",
    "# Create a new DataFrame with only the necessary columns\n",
    "test_df = test_df[['image_id', 'breast_birads']]\n",
    "\n",
    "# Reset the index for the DataFrame\n",
    "test_df = test_df.reset_index(drop=True)\n",
    "\n",
    "# Create the datasets\n",
    "test_dataset = VindrMammoDataset(bucket=bucket, dataframe=test_df, transform=transform)\n",
    "\n",
    "# Create the data loader\n",
    "valid_loader = torch.utils.data.DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=32,\n",
    "    shuffle=False,\n",
    "    num_workers=4,\n",
    "    pin_memory=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1WjQopHlBKWi"
   },
   "source": [
    "## Evaluate MobileNetV2\n",
    "For testing purposes we will first evaluate MobileNetV2 because it has reduced training times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 504
    },
    "id": "-Gviq3E_2nGd",
    "outputId": "6cccb9b2-fb18-4bc4-fb90-375d7d40bf49"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import torch\n",
    "from tqdm import tqdm  # Import tqdm for progress bar\n",
    "\n",
    "# Set model to evaluation mode\n",
    "mobilenetv2_fix_birads_model.eval()\n",
    "\n",
    "# Initialize lists to store predictions and true labels\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "\n",
    "# Disable gradient computation\n",
    "with torch.no_grad():\n",
    "    # Wrap the valid_loader with tqdm for progress bar\n",
    "    for images, labels in tqdm(valid_loader, desc=\"Evaluating\", total=len(valid_loader)):\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "        # Make predictions\n",
    "        outputs = mobilenetv2_fix_birads_model(images)\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "\n",
    "        # Store the predictions and true labels\n",
    "        all_preds.extend(preds.cpu().numpy())\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "# Compute the confusion matrix\n",
    "cm = confusion_matrix(all_labels, all_preds)\n",
    "\n",
    "# Plot the confusion matrix\n",
    "plt.figure(figsize=(6, 5))\n",
    "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=[\"Benign\", \"Malignant\"], yticklabels=[\"Benign\", \"Malignant\"])\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.title('Confusion Matrix for mobilenetv2_fix_birads_model')\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "A100",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
